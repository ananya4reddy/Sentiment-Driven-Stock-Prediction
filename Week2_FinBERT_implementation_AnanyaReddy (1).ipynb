{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d092e-7fff-4bed-a77c-e8809ff523d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132817f6-c536-4994-9945-47ba78b8bf39",
   "metadata": {},
   "source": [
    "# Week 2 : FinBERT Implementation & Challenges \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cd05e4e-8651-4397-a5f3-2bf1d1e0f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment probabilities [positive, negative, neutral]:\n",
      "tensor([[0.0090, 0.9754, 0.0156]])\n",
      "Average sentiment [Positive, Negative, Neutral]:\n",
      "tensor([[0.0090, 0.9685, 0.0225]])\n"
     ]
    }
   ],
   "source": [
    "## FinBERT sentiment analysis \n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# loading pretrained FinBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = BertForSequenceClassification.from_pretrained (\n",
    "    \"ProsusAI/finbert\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# starting with single financial sentence sentiment \n",
    "text = \"The company reported a 20% decline in revenue this quarter.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "print(\"Sentiment probabilities [positive, negative, neutral]:\")\n",
    "print(probabilities)\n",
    "\n",
    "\n",
    "# Long document handling, 512 token limit \n",
    "def finbert_long_text_sentiment(text, tokenizer, model, max_tokens=512):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    chunks = [\n",
    "        tokens[i:i + max_tokens]\n",
    "        for i in range(0, len(tokens), max_tokens)\n",
    "    ]\n",
    "    all_probs = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer.prepare_for_model(\n",
    "            chunk,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # adding batch dimensions because HuggingFace tokenizer creates batch dimension for single sentences automatically whereas for long text, we are adding \n",
    "        \n",
    "        inputs = {k: v.unsqueeze(0) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    avg_probs = torch.mean(torch.stack(all_probs), dim=0)\n",
    "    return avg_probs\n",
    "\n",
    "# testing long document sentiment \n",
    "long_text = \"\"\"\n",
    "The company announced its earnings today.\n",
    "Revenue declined due to rising operational costs.\n",
    "However, future guidance remains stable.\n",
    "\"\"\"\n",
    "\n",
    "result = finbert_long_text_sentiment(long_text, tokenizer, model)\n",
    "print(\"Average sentiment [Positive, Negative, Neutral]:\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e916a8-4bf9-4863-b229-a98b7268ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advanced Topic Challenges \n",
    "\n",
    "# Challenge 1 : Catastrophic forgetting \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "540da91c-c4b5-4d06-aab6-509086f14a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.8505281805992126}]\n",
      "-0.8505281805992126\n"
     ]
    }
   ],
   "source": [
    "## Advanced topic challenges \n",
    "\n",
    "# challenge 2 : the 512 token limit \n",
    "\n",
    "# Truncation Strategy \n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "document_text = \"\"\"\n",
    "The company reported strong revenue growth in the first quarter.\n",
    "However, rising inflation and supply chain disruptions posed risks.\n",
    "Management expects volatility in the upcoming fiscal year.\n",
    "\"\"\"\n",
    "\n",
    "# loading finbert sentiment analysis pipeline \n",
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\"\n",
    ")\n",
    "\n",
    "\n",
    "truncated_text = document_text[:512]           # keeping only the first 512 tokens \n",
    "sentiment = finbert(truncated_text)\n",
    "print(sentiment)\n",
    "\n",
    "\n",
    "# Hierarchical approach\n",
    "# here we split the document into multiple 512 token chunks and run finbert seperately on each chunk \n",
    "# and then combine the results as average or weighted \n",
    "\n",
    "tokens = tokenizer(document_text)\n",
    "\n",
    "# splitting tokens into chunks of 512\n",
    "chunk_size = 512\n",
    "chunks = []\n",
    "for i in range(0, len(tokens), 512):\n",
    "    chunk = document_text[i:i+chunk_size]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# finbert on each chunk \n",
    "sentiment_scores = []\n",
    "for chunk in chunks:\n",
    "    score = finbert(chunk)  # returns positive, negative, neutral scores\n",
    "    sentiment_scores.append(score)\n",
    "\n",
    "# combining the sentiment scores across all chunks \n",
    "\n",
    "label_map = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "weighted_scores = []\n",
    "\n",
    "for s in sentiment_scores:\n",
    "    label = s[0][\"label\"]\n",
    "    confidence = s[0][\"score\"]\n",
    "    weighted_scores.append(label_map[label] * confidence)\n",
    "\n",
    "final_sentiment = sum(weighted_scores) / len(weighted_scores)\n",
    "print(final_sentiment)\n",
    "\n",
    "\n",
    "# Longformer/ bigbird strategy \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53bdf1-b711-4cc9-922e-798123649532",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advanced topic challenges \n",
    "# challenge 3 : Slanted Triangular learning rates\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
